{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG with PDF üìÑ Data extraction to give context to LLM üß†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pypdf in d:\\study\\ai_study\\langchain_training\\myenv312\\lib\\site-packages (5.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load = load_dotenv('./../.env')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    model = \"qwen2.5:3b\",\n",
    "    temperature=0.5,\n",
    "    max_tokens = 250\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Extracting the PDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf1 = \"./attention.pdf\"\n",
    "pdf2 = \"./LLMForgetting.pdf\"\n",
    "pdf3 = \"./TestingAndEvaluatingLLM.pdf\"\n",
    "\n",
    "pdfFiles = [pdf1, pdf2, pdf3]\n",
    "\n",
    "documents = []\n",
    "\n",
    "for pdf in pdfFiles:\n",
    "    loader = PyPDFLoader(pdf)\n",
    "    documents.extend(loader.load())\n",
    "\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': './attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani‚àó\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer‚àó\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar‚àó\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit‚àó\\nGoogle Research\\nusz@google.com\\nLlion Jones‚àó\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez‚àó ‚Ä†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\n≈Åukasz Kaiser‚àó\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin‚àó ‚Ä°\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n‚àóEqual contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n‚Ä†Work performed while at Google Brain.\\n‚Ä°Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': './attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='1 Introduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht‚àí1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2')]\n"
     ]
    }
   ],
   "source": [
    "print(documents[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Text Splitting into Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "640"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, \n",
    "                                               chunk_overlap=200, add_start_index=True)\n",
    "\n",
    "all_splits = text_splitter.split_documents(documents)\n",
    "\n",
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3072, 3072)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"llama3.2:3b\")\n",
    "\n",
    "vector_1 = embeddings.embed_query(all_splits[0].page_content)\n",
    "vector_2 = embeddings.embed_query(all_splits[1].page_content)\n",
    "\n",
    "assert len(vector_1) == len(vector_2)\n",
    "len(vector_1), len(vector_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Vector Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -qU langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents = all_splits,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Retriving from the Persistant Vector Datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 CHAPTER 4. LOGICAL REASONING CORRECTNESS\n",
      "the following challenges: 1) If an LLM concludes correctly, it is unclear\n",
      "whether the response stems from reasoning or merely relies on simple\n",
      "heuristics such as memorization or word correlations (e.g., ‚Äúdry floor‚Äù\n",
      "is more likely to correlate with ‚Äúplaying football‚Äù). 2) If an LLM\n",
      "fails to reason correctly, it is not clear which part of the reasoning\n",
      "process it failed (i.e., inferring not raining from floor being dry or\n",
      "inferring playing football from not raining). 3) There is a lack of\n",
      "a system that can organize such test cases to cover all other formal\n",
      "reasoning scenarios besides implication, such as logical equivalence\n",
      "(e.g., If A then B, if B then A; therefore, A if and only if B). 4)\n",
      "Furthermore, understanding an LLM‚Äôs performance on such test cases\n",
      "provides little guidance on improving the reasoning ability of the\n",
      "LLM. To better handle these challenges, a well-performing testing\n",
      "60 CHAPTER 4. LOGICAL REASONING CORRECTNESS\n",
      "the following challenges: 1) If an LLM concludes correctly, it is unclear\n",
      "whether the response stems from reasoning or merely relies on simple\n",
      "heuristics such as memorization or word correlations (e.g., ‚Äúdry floor‚Äù\n",
      "is more likely to correlate with ‚Äúplaying football‚Äù). 2) If an LLM\n",
      "fails to reason correctly, it is not clear which part of the reasoning\n",
      "process it failed (i.e., inferring not raining from floor being dry or\n",
      "inferring playing football from not raining). 3) There is a lack of\n",
      "a system that can organize such test cases to cover all other formal\n",
      "reasoning scenarios besides implication, such as logical equivalence\n",
      "(e.g., If A then B, if B then A; therefore, A if and only if B). 4)\n",
      "Furthermore, understanding an LLM‚Äôs performance on such test cases\n",
      "provides little guidance on improving the reasoning ability of the\n",
      "LLM. To better handle these challenges, a well-performing testing\n",
      "60 CHAPTER 4. LOGICAL REASONING CORRECTNESS\n",
      "the following challenges: 1) If an LLM concludes correctly, it is unclear\n",
      "whether the response stems from reasoning or merely relies on simple\n",
      "heuristics such as memorization or word correlations (e.g., ‚Äúdry floor‚Äù\n",
      "is more likely to correlate with ‚Äúplaying football‚Äù). 2) If an LLM\n",
      "fails to reason correctly, it is not clear which part of the reasoning\n",
      "process it failed (i.e., inferring not raining from floor being dry or\n",
      "inferring playing football from not raining). 3) There is a lack of\n",
      "a system that can organize such test cases to cover all other formal\n",
      "reasoning scenarios besides implication, such as logical equivalence\n",
      "(e.g., If A then B, if B then A; therefore, A if and only if B). 4)\n",
      "Furthermore, understanding an LLM‚Äôs performance on such test cases\n",
      "provides little guidance on improving the reasoning ability of the\n",
      "LLM. To better handle these challenges, a well-performing testing\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "\n",
    "vector_store = Chroma(persist_directory='./chroma_langchain_db', embedding_function=embeddings)\n",
    "\n",
    "result = vector_store.similarity_search(\"What is Bias testing\", k=3)\n",
    "\n",
    "for doc in result:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Document(id='46fe7072-2837-4ea5-94ea-0a3342a41109', metadata={'author': '', 'creationdate': '2025-01-07T01:36:50+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2025-01-07T01:36:50+00:00', 'page': 9, 'page_label': '10', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': './LLMForgetting.pdf', 'start_index': 0, 'subject': '', 'title': '', 'total_pages': 15, 'trapped': '/False'}, page_content='Under review\\nFigure 6: The performance of general knowledge of the BLOOMZ-7.1b and LLAMA-7b\\nmodel trained on the instruction data and the mixed data. The dashed lines refers to the\\nperformance of BLOOMZ-7.1b and LLAMA-7B and the solid ones refer to those of mixed-\\ninstruction trained models.\\nincreases to 3b, BLOOMZ-3b suffers less forgetting compared to mT0-3.7B. For example, the\\nFG value of BLOOMZ-3b is 11.09 which is 5.64 lower than that of mT0-3.7b. These results\\nsuggest that BLOOMZ, which has a decoder-only model architecture, can maintain more\\nknowledge during continual instruction tuning. This difference may be attributed to the\\nautoregressive nature of the model or the differences in training objectives. Furthermore,\\nthe results imply that as the model scale increases, decoder-only models may suffer from\\nless catastrophic forgetting compared to encoder-decoder models. As we observe, the\\nknowledge degraded more drastically in mT0.\\n5.4 Effect of General Instruction Tuning'),\n",
       " 0.8928283452987671)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = vector_store.similarity_search_with_score(\"What are the types of LLM Testing\")\n",
    "\n",
    "result[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Retrivers in Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(id='79dfb367-7668-4d52-8327-45d2989d0bbf', metadata={'author': '', 'creationdate': '2024-10-22T16:30:56+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-10-22T16:30:56+00:00', 'page': 44, 'page_label': '36', 'producer': 'pdfTeX-1.40.26', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'source': './T2320074_Thesis Report.pdf', 'start_index': 0, 'subject': '', 'title': '', 'total_pages': 78, 'trapped': '/False'}, page_content='suggests two distinct design approaches. This data highlights a clear hierarchy of\\nroom importance and reveals how different spaces are prioritized in home designs,\\nwith some rooms being optional in many layouts.\\nThe final annotated graph data, selected chronologically due to resource constraints,\\nrepresents a more focused version of the initial filtered graphs, though some informa-\\ntionfromtheinitialdatasetmayhavebeenmissed. Whilethefinaldatasetmaintains\\nthe centrality of key rooms like the Living Room and its connections to spaces such\\nas the Master Room and Kitchen, it filters out less frequent interactions seen in the\\ninitial graphs. This results in a more concentrated view of room connectivity, but\\nthe chronological selection process may have overlooked certain patterns present in\\nthe broader dataset, potentially limiting the diversity of interactions captured.\\n3.7 Dataset Transformation and Refinement\\n3.7.1 Annotation Transformation')],\n",
       " [Document(id='cfdfa208-5549-42dc-b9ef-8fed7c261348', metadata={'author': '', 'creationdate': '2024-09-04T00:37:21+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-09-04T00:37:21+00:00', 'page': 76, 'page_label': '58', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': './TestingAndEvaluatingLLM.pdf', 'start_index': 772, 'subject': '', 'title': '', 'total_pages': 223, 'trapped': '/False'}, page_content='sive performance[89]. Some other works leverage LLMs‚Äô reasoning\\nability to repair programs automatically by providing semantically\\nsimilar bug fixes and reasoning hints[162]. However, many recent\\nstudies question the actual reasoning capacity of LLMs. For example,\\nrecent research by Google DeepMind [163] argues that LLMs cannot\\nself-correct reasoning, and another study suggests that LLMs are still\\nstruggling to address newly-created datasets despite their astonishing\\nperformance on well-known benchmark datasets [164].\\nLLMs with unreliable reasoning ability could induce severe conse-\\nquences in the real world. First, their problem-solving capabilities\\ncan be significantly impeded and it undermines the credibility of\\nmany downstream research and tools [89, 162, 165]. Second, it may\\npotentially generate inaccurate or misleading information, leading\\nusers to make uninformed decisions or develop misconceptions based\\non the flawed output from the LLMs. For example, LLaMA 2')],\n",
       " [Document(id='47c3535c-2bf8-46b3-8f8b-881f77c89ced', metadata={'author': '', 'creationdate': '2024-09-04T00:37:21+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-09-04T00:37:21+00:00', 'page': 102, 'page_label': '84', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': './TestingAndEvaluatingLLM.pdf', 'start_index': 1552, 'subject': '', 'title': '', 'total_pages': 223, 'trapped': '/False'}, page_content='noise when reading (e.g., ‚Äú*‚Äù in ‚ÄúH*ell*o‚Äù). Specifically, ‚ÄúHello‚Äù has\\nmultiple ‚Äúo‚Äùs, and ‚Äú*‚Äù is a mathematical symbol outside the English\\nalphabet. Therefore, humans can easily ignore the noises.\\nMR1-5 Character Masking\\nThis MR uses character masking, which masks a small portion')]]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs = {\"k\": 1}\n",
    ")\n",
    "\n",
    "retriever.batch(\n",
    "    [\n",
    "        \"What is the Bias Measurement\",\n",
    "        \"How to test human safety against LLM\",\n",
    "        \"How LLM forgets the context\"\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Retrival Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know.\n",
      "\n",
      "The provided context discusses reproducibility and comparability issues in evaluating Graph Neural Networks (GNN) models across various datasets. It mentions a study by Errica et al. (2020) where they re-evaluated five state-of-the-art GNN models on nine different benchmark data-sets from chemical and social domains. The key points include structure-agnostic baselines that outperform many traditional GNN models on specific types of datasets, such as D & D for chemical domains.\n",
      "\n",
      "The question \"What exactly does Testing the Factual Correctness of LLM tells?\" is not addressed in the provided context. Therefore, I don't have enough information to answer this question based on the given text.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "query = \"What exactly does Testing the Factual Correctness of LLM tells\"\n",
    "\n",
    "# To test if model is answering without context.\n",
    "#query = \"can burgers be made better with llms\"\n",
    "\n",
    "retrieved_docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "context_text = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are an AI Assisant. Use the following context to answer the question correctly.\n",
    "    If you dont know the answer, just tell, I dont know.\n",
    "    \n",
    "    Also, summarize the response in MD format\n",
    "    \n",
    "    \"context: {context} \\n\\n\"\n",
    "    \"question: {question} \\n\\n\"\n",
    "    \"AI answer:\n",
    "    \n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "chain = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "response = chain.invoke({\"context\": context_text, \"question\": query})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Langchain Hub for Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To test translation in an LLM (Large Language Model), you can add a specific instruction at the beginning of the prompt, such as \"Answering starts with 'Yes' or 'No'\". This helps guide the model on what type of response to produce. You then translate the expected response into English using Google Translate and feed both the translated prompt and the response back into the LLM for evaluation.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain import hub\n",
    "\n",
    "query = \"How to test Translation in LLM?\"\n",
    "\n",
    "retrieved_docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "context_text = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "response = chain.invoke({\"context\": context_text, \"question\": query})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving data using RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know. The context provided does not contain any information about testing the factual correctness of an Large Language Model (LLM).\n",
      "\n",
      "üìï Sources Used:\n",
      "- ./T2320074_Thesis Report.pdf\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm, retriever=retriever, return_source_documents=True)\n",
    "\n",
    "question = \"What exactly does Testing the Factual Correctness of LLM tells\"\n",
    "\n",
    "# To test if model is answering without context.\n",
    "#question = \"can burgers be made better with llms\"\n",
    "\n",
    "\n",
    "response = qa_chain.invoke(question)\n",
    "\n",
    "sources = set(doc.metadata.get(\"source\", \"Unknown\") for doc in response[\"source_documents\"])\n",
    "\n",
    "print(response['result'])\n",
    "print(\"\\nüìï Sources Used:\")\n",
    "for source in sources:\n",
    "    print(f\"- {source}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
