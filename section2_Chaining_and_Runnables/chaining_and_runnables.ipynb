{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain\n",
    "%pip install langchain-ollama\n",
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load ENV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load = load_dotenv('./../.env')\n",
    "\n",
    "print(os.getenv(\"LANGSMITH_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create LLM object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    model = \"qwen2.5:3b\",\n",
    "    temperature=0.5,\n",
    "    max_tokens = 250\n",
    ")\n",
    "\n",
    "llm2 = ChatOllama(\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    model = \"llama3.2:3b\",\n",
    "    temperature=0.5,\n",
    "    max_tokens = 250\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Chaining & Runnables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Running AI models on a local machine can offer several advantages, including:\\n\\n1. **Data Privacy**: Local processing keeps data within your organization's control, reducing concerns about data breaches or unauthorized access to sensitive information.\\n\\n2. **Faster Response Times**: Processing data locally reduces the latency associated with transmitting data over networks, which is particularly beneficial for real-time applications and interactive systems.\\n\\n3. **Cost Efficiency**: Running models on local hardware can be less expensive than maintaining cloud infrastructure, especially if you need to scale resources only when necessary.\\n\\n4. **Security Enhancements**: Local processing allows for more secure handling of critical information since it doesn't require transmitting data across potentially insecure networks.\\n\\n5. **Resource Utilization**: If the AI model is computationally intensive, running it locally can leverage your local hardware's capabilities, which might be better suited to specific tasks compared to cloud-based solutions.\\n\\n6. **Customization and Adaptation**: Local deployment gives you more flexibility in terms of customization and integration with existing systems or workflows.\\n\\n7. **Control Over Data Processing**: You have direct control over how the data is processed locally without exposing it to external environments, which can be crucial for compliance reasons.\\n\\n8. **Reduced Dependence on Internet Connectivity**: For critical applications that require continuous operation regardless of internet availability, local processing ensures uninterrupted service even when connectivity is poor or unavailable.\\n\\nWhile these advantages are significant, they come with trade-offs such as the need for more resources and expertise to set up a local environment and ensure it meets all necessary security standards.\", additional_kwargs={}, response_metadata={'model': 'qwen2.5:3b', 'created_at': '2025-04-12T14:03:53.0379159Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2386400000, 'load_duration': 16975300, 'prompt_eval_count': 30, 'prompt_eval_duration': 4198300, 'eval_count': 309, 'eval_duration': 2364692200, 'message': Message(role='assistant', content='', images=None, tool_calls=None), 'model_name': 'qwen2.5:3b'}, id='run-0c70eb8a-de25-4cd7-b016-5f4693317e79-0', usage_metadata={'input_tokens': 30, 'output_tokens': 309, 'total_tokens': 339})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate\n",
    ")\n",
    "\n",
    "# 1 way of doing it\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are an LLM expert\"),\n",
    "    (\"user\", \"What is the advantage of running AI Models in {env}\")\n",
    "])\n",
    "\n",
    "# Without Chaining\n",
    "# prompt = prompt_template.invoke({\"env\": \"local machine\"})\n",
    "\n",
    "# content = llm.invoke(prompt).content\n",
    "\n",
    "# Chaining mechanism\n",
    "chain = prompt_template | llm \n",
    "\n",
    "chain.invoke({\"env\": \"local machine\"})\n",
    "\n",
    "#print(chain)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running AI models on a local machine, also known as edge computing or client-side processing, offers several advantages compared to cloud-based solutions. Here are some key benefits:\n",
      "\n",
      "1. **Data Privacy and Security**: By keeping data processing within the local environment, you can better control what happens with sensitive information. This is particularly important in industries where privacy laws like GDPR or CCPA apply.\n",
      "\n",
      "2. **Latency Reduction**: Local models reduce network latency because they process data without sending it to a remote server. This can be crucial for applications requiring real-time decision-making and low-latency interactions, such as voice recognition or augmented reality.\n",
      "\n",
      "3. **Cost Efficiency**: Processing local AI models reduces the need for expensive bandwidth usage and potentially lowers cloud costs by reducing the load on central servers.\n",
      "\n",
      "4. **Resource Optimization**: Local processing allows you to use available resources more efficiently. This can be particularly beneficial in resource-constrained environments, such as IoT devices or edge computing nodes.\n",
      "\n",
      "5. **Customization and Flexibility**: Developers have more control over the environment where their AI models operate. They can tailor it to specific requirements without needing to rely on a cloud provider's infrastructure.\n",
      "\n",
      "6. **Operational Control**: With local processing, you maintain full control over updates, patches, and security measures for your AI model, which is especially important in critical applications like cybersecurity or healthcare diagnostics.\n",
      "\n",
      "7. **Scalability and Flexibility**: Local models can be scaled more easily based on the specific needs of different users or use cases without needing to scale entire cloud infrastructure.\n",
      "\n",
      "8. **Redundancy and Resilience**: Having local copies of AI models can provide redundancy in case of network failures, ensuring that critical applications continue to function even if there are issues with remote servers.\n",
      "\n",
      "These advantages make running AI models on a local machine an attractive option for various scenarios where data privacy, real-time processing, cost considerations, or specific application requirements come into play.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are an LLM expert\"),\n",
    "    (\"user\", \"What is the advantage of running AI Models in {env}\")\n",
    "])\n",
    "\n",
    "chain = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "response = chain.invoke({\"env\": \"local machine\"})\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chaining Multiple Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- **Data Privacy and Security**\n",
      "- **Latency Reduction**\n",
      "- **Cost Efficiency**\n",
      "- **Control Over Data**\n",
      "- **Resilience**\n",
      "- **Customization**\n",
      "- **Resource Efficiency**\n",
      "- **Energy Consumption**\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are an LLM expert\"),\n",
    "    (\"user\", \"What is the advantage of running AI Models in {env}\")\n",
    "])\n",
    "\n",
    "# Chain 1\n",
    "detailedResponseChain = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "headingInfoTemplate = ChatPromptTemplate.from_template(\"\"\"\n",
    "                                         Analyse the response and get me just the heading from the {response}\n",
    "                                         \n",
    "                                         Response should be in bullet points\n",
    "                                         \"\"\")\n",
    "\n",
    "# Chain 2\n",
    "chainWithHeading = {\"response\": detailedResponseChain } | headingInfoTemplate | llm | StrOutputParser()\n",
    "\n",
    "response = chainWithHeading.invoke({\"env\": \"local machine\"})\n",
    "\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running chains in Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running AI models on a local machine, also known as edge computing or decentralized processing, offers several advantages compared to cloud-based solutions. Here are some key benefits:\n",
      "\n",
      "1. **Data Privacy and Security**: Storing sensitive data locally reduces the risk of data breaches and unauthorized access. This is particularly important for industries with strict regulations regarding data privacy, such as healthcare or finance.\n",
      "\n",
      "2. **Latency Reduction**: Local execution allows AI models to run faster since there's no need to send data over a network. This can be crucial in applications where real-time decision-making is necessary (e.g., autonomous vehicles).\n",
      "\n",
      "3. **Cost Efficiency**: Lower bandwidth usage and the ability to perform tasks without internet connectivity can lead to cost savings, especially for intermittent or low-bandwidth connections.\n",
      "\n",
      "4. **Resource Management**: Local execution allows you to manage resources more efficiently by not relying on centralized cloud services that may have limited available resources at any given time.\n",
      "\n",
      "5. **Customization and Adaptability**: Models running locally can be tailored to specific needs without having to rely on external, potentially slow or unreliable cloud infrastructure. This is particularly useful for small businesses or organizations with unique requirements.\n",
      "\n",
      "6. **Consistency in Data Handling**: Local execution ensures that data processing happens consistently across different environments (e.g., office settings vs. mobile devices) as long as the model and its dependencies are available locally.\n",
      "\n",
      "7. **Energy Efficiency**: In some cases, local execution can lead to energy savings if it reduces the need for frequent communication with remote servers.\n",
      "\n",
      "8. **Security of Models**: Keeping AI models on a local machine ensures that they remain secure from potential vulnerabilities or attacks in cloud environments.\n",
      "\n",
      "However, running AI models locally also comes with challenges such as increased complexity in managing and updating models, specialized hardware requirements (like GPUs), and the need for more rigorous security measures to protect data.\n",
      "\n",
      "\n",
      "\n",
      "Here are the 8 key benefits of running AI models on a local machine:\n",
      "\n",
      "• **Data Privacy and Security**: Data remains within your organization's infrastructure, reducing concerns about data breaches or unauthorized access.\n",
      "• **Latency Reduction**: Local processing reduces time delay associated with transmitting data over networks to a cloud service and back.\n",
      "• **Cost Efficiency**: Running AI models locally often involves lower costs due to avoided usage fees or subscription charges.\n",
      "• **Resource Management**: Fine-tuning resource allocation specifically for your AI models without worrying about managing a shared environment.\n",
      "• **Customization and Integration**: Having the model on your local machine provides flexibility in terms of customization and seamless integration into existing systems or workflows.\n",
      "• **Control Over Environment**: You have complete control over the environment where your AI models run, including operating system versions, libraries, and dependencies.\n",
      "• **Backup and Recovery**: Local models are easier to back up and recover from failures or data loss compared to cloud-based solutions.\n",
      "• **Security Measures**: Implementing additional security measures specific to your local environment, such as firewalls, intrusion detection systems, and secure communication channels.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are an LLM expert\"),\n",
    "    (\"user\", \"What is the advantage of running AI Models in {env}\")\n",
    "])\n",
    "\n",
    "# Chain 1\n",
    "detailedResponseChain = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "headingInfoTemplate = ChatPromptTemplate.from_template(\"\"\"\n",
    "                                         Analyse the response and get me just the heading from the {response}\n",
    "                                         \n",
    "                                         Response should be in bullet points\n",
    "                                         \"\"\")\n",
    "\n",
    "# Chain 2\n",
    "chainWithHeading = {\"response\": detailedResponseChain } | headingInfoTemplate | llm2 | StrOutputParser()\n",
    "\n",
    "parallelRunnable = RunnableParallel(chain1=detailedResponseChain, chain2=chainWithHeading)\n",
    "\n",
    "response = parallelRunnable.invoke({\"env\": \"local machine\"})\n",
    "\n",
    "print(response['chain1'])\n",
    "print(\"\\n\\n\")\n",
    "print(response['chain2'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running AI models on a local machine, rather than relying solely on cloud-based services, offers several advantages. Here are some key benefits:\n",
      "\n",
      "1. **Data Privacy and Security**: Storing and processing data locally reduces the risk of sensitive information being exposed to external servers or networks, which can be especially important in regulated industries like healthcare or finance.\n",
      "\n",
      "2. **Performance**: Local execution often leads to faster results because there is no delay introduced by network latency between the local machine and cloud servers. This can be crucial for real-time applications where quick response times are essential.\n",
      "\n",
      "3. **Cost Efficiency**: While initial setup costs may be higher, running AI models locally can lead to long-term cost savings, especially if you have a significant amount of data that needs frequent processing or analysis. Cloud services often charge based on usage, which can become expensive for large-scale operations.\n",
      "\n",
      "4. **Control and Customization**: You have more control over the environment where your AI model runs. This allows for fine-tuning configurations to suit specific requirements, such as custom libraries, hardware specifications, etc., without being limited by cloud provider policies.\n",
      "\n",
      "5. **Disaster Recovery and Business Continuity**: If a critical system goes down due to connectivity issues or other network problems, having the ability to run AI models locally ensures that you can continue operations even if external services are unavailable.\n",
      "\n",
      "6. **Data Handling**: For certain applications involving large volumes of data, local processing can be more efficient as it doesn't require transmitting all data over a potentially slow or unreliable network connection.\n",
      "\n",
      "7. **Security and Compliance**: In some cases, running AI models locally can help meet compliance requirements that dictate how sensitive information must be handled (e.g., GDPR for EU regulations).\n",
      "\n",
      "While these advantages are compelling, they also come with considerations such as the need to manage hardware resources, ensure data security, and maintain software dependencies up-to-date. Therefore, choosing whether to run AI models on a local machine should depend on your specific needs, including the nature of the application, available resources, and regulatory requirements.\n",
      "\n",
      "\n",
      "\n",
      "There are several advantages of running a Large Language Model (LLM) on a cloud machine:\n",
      "\n",
      "1. **Scalability**: Cloud machines can be scaled up or down as needed, allowing you to easily increase or decrease the power and resources allocated to your LLM.\n",
      "2. **High-performance computing**: Cloud providers offer high-performance computing (HPC) instances that are optimized for deep learning workloads like LLMs.\n",
      "3. **Access to specialized hardware**: Cloud providers offer access to specialized hardware such as NVIDIA GPUs, TPUs, and FPGAs, which are well-suited for deep learning workloads.\n",
      "4. **Pre-configured environments**: Cloud providers offer pre-configured environments with optimized software stacks, frameworks, and libraries, making it easier to get started with LLM development.\n",
      "5. **Cost-effectiveness**: Running an LLM on a cloud machine can be more cost-effective than setting up and maintaining your own infrastructure, especially for small-scale or proof-of-concept projects.\n",
      "6. **Flexibility**: Cloud machines can be easily spun up and down as needed, allowing you to quickly experiment with different architectures, hyperparameters, and models without incurring significant costs.\n",
      "7. **Access to advanced features**: Cloud providers offer advanced features such as automatic model updates, versioning, and collaboration tools, making it easier to manage and share your LLM development workflow.\n",
      "8. **Reduced maintenance**: Cloud providers handle the maintenance of the underlying infrastructure, freeing you up to focus on developing and improving your LLM.\n",
      "9. **Access to large datasets**: Cloud providers often offer access to large datasets and libraries that can be used for training and testing your LLM.\n",
      "10. **Faster iteration**: Running an LLM on a cloud machine allows for faster iteration and experimentation, as you can quickly spin up new instances and try out different configurations.\n",
      "\n",
      "Some popular cloud platforms for running LLMs include:\n",
      "\n",
      "* Amazon Web Services (AWS)\n",
      "* Microsoft Azure\n",
      "* Google Cloud Platform (GCP)\n",
      "* IBM Cloud\n",
      "\n",
      "Each platform has its own strengths and weaknesses, and the choice of which one to use will depend on your specific needs and requirements.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "localMachineTemplate = ChatPromptTemplate([\n",
    "    (\"system\", \"You are an LLM expert\"),\n",
    "    (\"user\", \"What is the advantage of running AI Models in {env}\")\n",
    "])\n",
    "\n",
    "# Chain 1\n",
    "localMachineChain = localMachineTemplate | llm | StrOutputParser()\n",
    "\n",
    "cloudMachineTemplate = ChatPromptTemplate.from_template(\"\"\"\n",
    "                                         What is the advantage of running LLM in {machine}\n",
    "                                         \"\"\")\n",
    "\n",
    "# Chain 2\n",
    "cloudMachineChain = cloudMachineTemplate | llm2 | StrOutputParser()\n",
    "\n",
    "parallelRunnable = RunnableParallel(chain1=localMachineChain, chain2=cloudMachineChain)\n",
    "\n",
    "response = parallelRunnable.invoke({\"env\": \"local machine\", \"machine\": \"cloud machine\"})\n",
    "\n",
    "print(response['chain1'])\n",
    "print(\"\\n\\n\")\n",
    "print(response['chain2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runnable Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- **Data Privacy and Security**\n",
      "- **Faster Response Times**\n",
      "- **Cost Efficiency**\n",
      "- **Customization Flexibility**\n",
      "- **Control Over Environment**\n",
      "- **Reduced Dependence on Internet Connectivity**\n",
      "- **Security of Local Data**\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are an LLM expert\"),\n",
    "    (\"user\", \"What is the advantage of running AI Models in {env}\")\n",
    "])\n",
    "\n",
    "# Chain 1\n",
    "detailedResponseChain = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "headingInfoTemplate = ChatPromptTemplate.from_template(\"\"\"\n",
    "                                         Analyse the response and get me just the heading from the {response}\n",
    "                                         \n",
    "                                         Response should be in bullet points\n",
    "                                         \"\"\")\n",
    "def choose_llm(response):\n",
    "    response_text = str(response)\n",
    "    if len(response_text) < 300:\n",
    "        return llm2\n",
    "    return llm\n",
    "\n",
    "llm_selector = RunnableLambda(choose_llm)\n",
    "\n",
    "\n",
    "# Chain 2\n",
    "chainWithHeading = {\"response\": detailedResponseChain } | headingInfoTemplate | choose_llm | StrOutputParser()\n",
    "\n",
    "response = chainWithHeading.invoke({\"env\": \"local machine\"})\n",
    "\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using @Chain decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- **Latency Reduction**\n",
      "- **Security**\n",
      "- **Bandwidth Savings**\n",
      "- **Data Privacy**\n",
      "- **Cost Efficiency**\n",
      "- **Resource Utilization**\n",
      "- **Consistent Performance**\n",
      "- **Customization**\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are an LLM expert\"),\n",
    "    (\"user\", \"What is the advantage of running AI Models in {env}\")\n",
    "])\n",
    "\n",
    "# Chain 1\n",
    "detailedResponseChain = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "headingInfoTemplate = ChatPromptTemplate.from_template(\"\"\"\n",
    "                                         Analyse the response and get me just the heading from the {response}\n",
    "                                         \n",
    "                                         Response should be in bullet points\n",
    "                                       \"\"\")\n",
    "@chain\n",
    "def choose_llm(response):\n",
    "    response_text = str(response)\n",
    "    if len(response_text) < 300:\n",
    "        return llm2\n",
    "    return llm\n",
    "\n",
    "# Chain 2\n",
    "chainWithHeading = {\"response\": detailedResponseChain } | headingInfoTemplate | choose_llm | StrOutputParser()\n",
    "\n",
    "response = chainWithHeading.invoke({\"env\": \"local machine\"})\n",
    "\n",
    "print(response)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
