{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fb85555",
   "metadata": {},
   "source": [
    "### Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "251e748a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in d:\\study\\ai_study\\langchain_training\\myenv312\\lib\\site-packages (0.3.23)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in d:\\study\\ai_study\\langchain_training\\myenv312\\lib\\site-packages (from langchain) (0.3.51)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in d:\\study\\ai_study\\langchain_training\\myenv312\\lib\\site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in d:\\study\\ai_study\\langchain_training\\myenv312\\lib\\site-packages (from langchain) (0.3.30)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in d:\\study\\ai_study\\langchain_training\\myenv312\\lib\\site-packages (from langchain) (2.11.3)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in d:\\study\\ai_study\\langchain_training\\myenv312\\lib\\site-packages (from langchain) (2.0.40)\n",
      "Requirement already satisfied: requests<3,>=2 in d:\\study\\ai_study\\langchain_training\\myenv312\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\study\\ai_study\\langchain_training\\myenv312\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in d:\\study\\ai_study\\langchain_training\\myenv312\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\study\\ai_study\\langchain_training\\myenv312\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in d:\\study\\ai_study\\langchain_training\\myenv312\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in d:\\study\\ai_study\\langchain_training\\myenv312\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (4.13.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\study\\ai_study\\langchain_training\\myenv312\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\study\\ai_study\\langchain_training\\myenv312\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in d:\\study\\ai_study\\langchain_training\\myenv312\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in d:\\study\\ai_study\\langchain_training\\myenv312\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\study\\ai_study\\langchain_training\\myenv312\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in d:\\study\\ai_study\\langchain_training\\myenv312\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\study\\ai_study\\langchain_training\\myenv312\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\study\\ai_study\\langchain_training\\myenv312\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\study\\ai_study\\langchain_training\\myenv312\\lib\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\study\\ai_study\\langchain_training\\myenv312\\lib\\site-packages (from requests<3,>=2->langchain) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\study\\ai_study\\langchain_training\\myenv312\\lib\\site-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
      "Requirement already satisfied: greenlet>=1 in d:\\study\\ai_study\\langchain_training\\myenv312\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: anyio in d:\\study\\ai_study\\langchain_training\\myenv312\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\study\\ai_study\\langchain_training\\myenv312\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\study\\ai_study\\langchain_training\\myenv312\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\study\\ai_study\\langchain_training\\myenv312\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain) (3.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\study\\ai_study\\langchain_training\\myenv312\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-ollama in d:\\study\\ai_study\\langchain_training\\myenv312\\lib\\site-packages (0.3.1)\n",
      "Requirement already satisfied: ollama<1,>=0.4.4 in d:\\study\\ai_study\\langchain_training\\myenv312\\lib\\site-packages (from langchain-ollama) (0.4.7)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in d:\\study\\ai_study\\langchain_training\\myenv312\\lib\\site-packages (from langchain-ollama) (0.3.51)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in d:\\study\\ai_study\\langchain_training\\myenv312\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain-ollama) (0.3.30)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in d:\\study\\ai_study\\langchain_training\\myenv312\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain-ollama) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\study\\ai_study\\langchain_training\\myenv312\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain-ollama) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\study\\ai_study\\langchain_training\\myenv312\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain-ollama) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in d:\\study\\ai_study\\langchain_training\\myenv312\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain-ollama) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in d:\\study\\ai_study\\langchain_training\\myenv312\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain-ollama) (4.13.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in d:\\study\\ai_study\\langchain_training\\myenv312\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain-ollama) (2.11.3)\n",
      "Requirement already satisfied: httpx<0.29,>=0.27 in d:\\study\\ai_study\\langchain_training\\myenv312\\lib\\site-packages (from ollama<1,>=0.4.4->langchain-ollama) (0.28.1)\n",
      "Requirement already satisfied: anyio in d:\\study\\ai_study\\langchain_training\\myenv312\\lib\\site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (4.9.0)\n",
      "Requirement already satisfied: certifi in d:\\study\\ai_study\\langchain_training\\myenv312\\lib\\site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\study\\ai_study\\langchain_training\\myenv312\\lib\\site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (1.0.7)\n",
      "Requirement already satisfied: idna in d:\\study\\ai_study\\langchain_training\\myenv312\\lib\\site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\study\\ai_study\\langchain_training\\myenv312\\lib\\site-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\study\\ai_study\\langchain_training\\myenv312\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain-ollama) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\study\\ai_study\\langchain_training\\myenv312\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.51->langchain-ollama) (3.10.16)\n",
      "Requirement already satisfied: requests<3,>=2 in d:\\study\\ai_study\\langchain_training\\myenv312\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.51->langchain-ollama) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in d:\\study\\ai_study\\langchain_training\\myenv312\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.51->langchain-ollama) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in d:\\study\\ai_study\\langchain_training\\myenv312\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.51->langchain-ollama) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\study\\ai_study\\langchain_training\\myenv312\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<1.0.0,>=0.3.51->langchain-ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in d:\\study\\ai_study\\langchain_training\\myenv312\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<1.0.0,>=0.3.51->langchain-ollama) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\study\\ai_study\\langchain_training\\myenv312\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<1.0.0,>=0.3.51->langchain-ollama) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\study\\ai_study\\langchain_training\\myenv312\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.51->langchain-ollama) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\study\\ai_study\\langchain_training\\myenv312\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.51->langchain-ollama) (2.4.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\study\\ai_study\\langchain_training\\myenv312\\lib\\site-packages (from anyio->httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in d:\\study\\ai_study\\langchain_training\\myenv312\\lib\\site-packages (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain\n",
    "%pip install langchain-ollama\n",
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5678b2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -U langchain langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a62ba9",
   "metadata": {},
   "source": [
    "### Load ENV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02200ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load = load_dotenv('./../.env')\n",
    "\n",
    "print(os.getenv(\"LANGSMITH_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71bedf9",
   "metadata": {},
   "source": [
    "### Interacting with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46d6b841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='<think>\\nOkay, so I just got a message from someone saying \"hello, How are you doing?\" and then the assistant responded with a greeting. I\\'m trying to figure out how to respond properly. Let me think through this step by step.\\n\\nFirst, when someone greets me with a simple \"hello,\" it\\'s usually polite to return the greeting. So I should probably say something like \"Hello!\" That seems standard.\\n\\nNext, they\\'re asking \"How are you doing?\" That\\'s a common question used to inquire about someone\\'s well-being or current state. The assistant responded by saying it was doing well and asked how I am. So I should follow that pattern.\\n\\nI might want to add a bit more to make the conversation flow naturally. Maybe respond with how I am doing, then ask a question in return. That way, the conversation continues. For example: \"Hello! I\\'m doing well, thank you. How can I assist you today?\"\\n\\nWait, should I include an exclamation mark? The assistant used one, so maybe that\\'s appropriate for a casual greeting.\\n\\nAlso, considering the context, since this is a chat, keeping it concise but friendly makes sense. So putting it all together: \"Hello! I\\'m doing well, thank you. How can I assist you today?\"\\n\\nI think that covers the greeting, acknowledges their question, and opens the door for further conversation. Is there anything else I should include? Maybe not necessary unless I want to add more personalization.\\n\\nAlternatively, if I wanted to be a bit more expressive, I could say something like \"Hello! I\\'m doing great, thanks for asking. How can I help you today?\" That\\'s also good and keeps it friendly without being too verbose.\\n\\nHmm, but maybe the first version is better because it\\'s concise. It allows the conversation to progress smoothly without any unnecessary words.\\n\\nAnother thought: should I use a period or exclamation mark? The assistant used an exclamation mark, so probably that\\'s acceptable in this context. It makes the greeting feel warmer.\\n\\nSo, finalizing my response as \"Hello! I\\'m doing well, thank you. How can I assist you today?\" seems appropriate.\\n</think>\\n\\nHello! I\\'m doing well, thank you. How can I assist you today?' additional_kwargs={} response_metadata={'model': 'deepseek-r1:8b', 'created_at': '2025-04-12T11:57:24.0721301Z', 'done': True, 'done_reason': 'stop', 'total_duration': 17073871500, 'load_duration': 11611292400, 'prompt_eval_count': 10, 'prompt_eval_duration': 350098200, 'eval_count': 462, 'eval_duration': 5106929200, 'message': Message(role='assistant', content='', images=None, tool_calls=None), 'model_name': 'deepseek-r1:8b'} id='run-b8dc22d9-611d-4856-994b-6e3ab60843d9-0' usage_metadata={'input_tokens': 10, 'output_tokens': 462, 'total_tokens': 472}\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm =  ChatOllama(\n",
    "    base_url = \"http://localhost:11434\",\n",
    "    model = \"deepseek-r1:8b\",\n",
    "    temperature= 0.5 ,\n",
    "    max_tokens = 1000\n",
    ")\n",
    "\n",
    "response = llm.invoke(\"hello, How are you doing?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1661732",
   "metadata": {},
   "source": [
    "#### Structured output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1541fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending prompt: \"What are AI agents? What is the difference between AI, AI agents, AI workflow, automation etc\"\n",
      "\n",
      "==================================================\n",
      "--- Thinking Process ---\n",
      "==================================================\n",
      "Okay, so I'm trying to understand what AI agents are. From what I remember, AI stands for Artificial Intelligence, which is about machines being able to perform tasks that usually require human intelligence. But then there's this term \"AI agent,\" and I'm not entirely sure how it fits into all of this.\n",
      "\n",
      "I think an AI agent might be some kind of software program, right? Maybe like a robot or something that can make decisions on its own. But wait, isn't AI more about learning and adapting, while agents are more like autonomous entities? Hmm, maybe I should break it down further.\n",
      "\n",
      "So, if AI is the broader field, then AI agents must be a specific type of system within AI. Maybe they're like the ones that take inputs and process them to make decisions or actions without human intervention. For example, chatbots are AI agents because they interact with users automatically.\n",
      "\n",
      "Then there's AI workflow. I'm guessing this has to do with how AI systems handle tasks in sequence. Like, if you have a system that automates several steps, the workflow would manage the order and flow of those steps. So, an agent might be part of a larger workflow where it handles specific tasks or interactions.\n",
      "\n",
      "Automation is another term I've heard a lot. It's about making processes more efficient by reducing manual intervention. So, AI automation probably uses AI to handle repetitive or complex tasks that would otherwise require humans. That makes sense because AI can process data quickly and accurately, which is perfect for automating things.\n",
      "\n",
      "Now, trying to figure out the differences between these terms:\n",
      "\n",
      "1. **AI vs. AI Agents**: AI is the field, while AI agents are the entities within it. So, an AI agent is like a specific instance of AI that can perform tasks autonomously.\n",
      "\n",
      "2. **AI Agents vs. Workflow**: Workflow seems to be about how tasks are organized and executed. An AI workflow would involve multiple AI agents working together in a sequence to achieve a larger goal. So, the agent is part of the workflow, but the workflow is more about the structure of the process.\n",
      "\n",
      "3. **Automation vs. AI Automation**: Automation is a broader concept that includes both traditional methods (like mechanical automation) and technological ones. AI automation specifically refers to using AI technologies to automate tasks. So, it's a subset of automation that leverages AI capabilities.\n",
      "\n",
      "Wait, but I'm not sure if I got that right. Let me think again. If automation can be done without AI, then AI automation is just one type of automation. That makes sense because there are other ways to automate processes before AI became prevalent.\n",
      "\n",
      "So, putting it all together:\n",
      "\n",
      "- **AI**: The field focused on creating intelligent systems.\n",
      "- **AI Agents**: Individual entities within AI that can perform tasks and make decisions autonomously.\n",
      "- **AI Workflow**: The structured sequence of tasks or actions managed by AI, often involving multiple agents working together.\n",
      "- **Automation**: The broader concept of making processes efficient through technology, including AI automation which uses AI to handle tasks.\n",
      "\n",
      "I think I have a better grasp now. AI agents are like the actors within the AI system, while workflows manage how they interact and perform tasks. Automation is the overall efficiency improvement, with AI automation being a specific method.\n",
      "\n",
      "\n",
      "==================================================\n",
      "--- Main Response ---\n",
      "==================================================\n",
      "**Understanding AI Agents and Related Concepts**\n",
      "\n",
      "1. **Artificial Intelligence (AI)**:\n",
      "   - The field focused on creating intelligent systems that can perform tasks typically requiring human intelligence.\n",
      "\n",
      "2. **AI Agents**:\n",
      "   - Autonomous entities within AI that process inputs to make decisions or take actions without human intervention. Examples include chatbots and recommendation systems.\n",
      "\n",
      "3. **AI Workflow**:\n",
      "   - The structured sequence of tasks managed by AI, often involving multiple agents working together to achieve a larger goal. It's the organization and execution structure of tasks.\n",
      "\n",
      "4. **Automation**:\n",
      "   - The broader concept of enhancing efficiency through technology. AI automation specifically refers to using AI technologies to automate tasks, which is a subset of automation that leverages AI capabilities.\n",
      "\n",
      "**Key Differences**:\n",
      "\n",
      "- **AI vs. AI Agents**: AI is the field; AI agents are specific entities within it.\n",
      "- **AI Agents vs. Workflow**: AI agents are part of the workflow, which manages task sequences and structures.\n",
      "- **Automation vs. AI Automation**: Automation is broader, including traditional methods, while AI automation specifically uses AI technologies.\n",
      "\n",
      "In summary, AI agents are integral to AI systems, working within structured workflows to automate tasks efficiently.\n",
      "\n",
      "\n",
      "==================================================\n",
      "--- Response Metadata ---\n",
      "==================================================\n",
      "{\n",
      "    \"model\": \"deepseek-r1:8b\",\n",
      "    \"created_at\": \"2025-04-12T00:21:56.5065922Z\",\n",
      "    \"done\": true,\n",
      "    \"done_reason\": \"stop\",\n",
      "    \"total_duration\": 10322892200,\n",
      "    \"load_duration\": 15691500,\n",
      "    \"prompt_eval_count\": 23,\n",
      "    \"prompt_eval_duration\": 46744600,\n",
      "    \"eval_count\": 908,\n",
      "    \"eval_duration\": 10260013300,\n",
      "    \"message\": \"role='assistant' content='' images=None tool_calls=None\",\n",
      "    \"model_name\": \"deepseek-r1:8b\"\n",
      "}\n",
      "\n",
      "\n",
      "==================================================\n",
      "--- Usage Metadata ---\n",
      "==================================================\n",
      "{\n",
      "    \"input_tokens\": 23,\n",
      "    \"output_tokens\": 908,\n",
      "    \"total_tokens\": 931\n",
      "}\n",
      "\n",
      "\n",
      "==================================================\n",
      "--- Other Info ---\n",
      "==================================================\n",
      "{\n",
      "    \"id\": \"run-d806d147-2fc0-4327-a654-6f932650031c-0\",\n",
      "    \"additional_kwargs\": {},\n",
      "    \"type\": \"ai\"\n",
      "}\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import AIMessage # Import AIMessage for type hinting (optional but good practice)\n",
    "\n",
    "# --- Initialize the LLM (same as before) ---\n",
    "llm =  ChatOllama(\n",
    "    base_url = \"http://localhost:11434\",\n",
    "    model = \"deepseek-r1:8b\",\n",
    "    temperature= 0.5\n",
    ")\n",
    "\n",
    "# --- Invoke the LLM (same as before) ---\n",
    "prompt = \"What are AI agents? What is the difference between AI, AI agents, AI workflow, automation etc\"\n",
    "print(f\"Sending prompt: \\\"{prompt}\\\"\\n\")\n",
    "response: AIMessage = llm.invoke(prompt) # Added type hint\n",
    "\n",
    "\n",
    "# --- Structure and Visualize the Output ---\n",
    "\n",
    "# 1. Extract the full content string\n",
    "full_content = response.content\n",
    "\n",
    "# 2. Separate the <think> block and the main response using regex\n",
    "think_content = \"N/A\" # Default if not found\n",
    "main_response = full_content # Default if no think block\n",
    "\n",
    "# Use regex to find the think block (dot matches newline)\n",
    "# Assuming the model *might* output <think> tags based on the original code's intent\n",
    "# If your model doesn't use <think>, this part might not find anything, which is fine.\n",
    "match = re.search(r\"<think>(.*?)</think>\", full_content, re.DOTALL)\n",
    "if match:\n",
    "    think_content = match.group(1).strip() # Get content inside tags\n",
    "    # Find the end of the </think> tag to get the main response after it\n",
    "    end_think_tag_index = match.end()\n",
    "    main_response = full_content[end_think_tag_index:].strip()\n",
    "else:\n",
    "    # If no <think> block, assume the whole content is the main response\n",
    "    # and the thinking process is not explicitly separated in the output.\n",
    "    main_response = full_content.strip()\n",
    "    think_content = \"(No <think> block found in response)\"\n",
    "\n",
    "\n",
    "# 3. Extract Metadata\n",
    "response_meta = response.response_metadata\n",
    "usage_meta = response.usage_metadata\n",
    "other_info = {\n",
    "    \"id\": response.id,\n",
    "    \"additional_kwargs\": response.additional_kwargs,\n",
    "    # Add type if needed, though it's usually clear from context\n",
    "    \"type\": response.type\n",
    "}\n",
    "\n",
    "# --- FIX: Define a default serializer function for json.dumps ---\n",
    "def default_serializer(obj):\n",
    "    \"\"\"Convert non-serializable objects to strings.\"\"\"\n",
    "    try:\n",
    "        # Let the default encoder try first\n",
    "        return json.JSONEncoder().default(obj)\n",
    "    except TypeError:\n",
    "        # If it fails, convert the object to its string representation\n",
    "        return str(obj)\n",
    "\n",
    "# 4. Print the structured output\n",
    "print(\"=\"*50)\n",
    "print(\"--- Thinking Process ---\")\n",
    "print(\"=\"*50)\n",
    "print(think_content)\n",
    "print(\"\\n\") # Add a newline for spacing\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"--- Main Response ---\")\n",
    "print(\"=\"*50)\n",
    "print(main_response)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"--- Response Metadata ---\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# --- FIX: Use the default serializer in json.dumps ---\n",
    "print(json.dumps(response_meta, indent=4, default=default_serializer))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"--- Usage Metadata ---\")\n",
    "print(\"=\"*50)\n",
    "# Assuming usage_meta is already JSON serializable (usually is)\n",
    "# If you get an error here too, apply the same default=default_serializer fix\n",
    "print(json.dumps(usage_meta, indent=4))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"--- Other Info ---\")\n",
    "print(\"=\"*50)\n",
    "# other_info should be serializable as constructed\n",
    "print(json.dumps(other_info, indent=4))\n",
    "print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915e6a20",
   "metadata": {},
   "source": [
    "### Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d999e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\"What is the advantage of running the LLM in {env}\");\n",
    "\n",
    "prompt = prompt_template.invoke({\"env\": \"local machine\"})\n",
    "\n",
    "print(prompt)\n",
    "\n",
    "content = llm.invoke(prompt).content\n",
    "\n",
    "\n",
    "\n",
    "#print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28dcc18",
   "metadata": {},
   "source": [
    "### Chat Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd2ca8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate\n",
    ")\n",
    "\n",
    "systemMessage = SystemMessagePromptTemplate.from_template(\"You are an LLM expert\")\n",
    "humanMessage = HumanMessagePromptTemplate.from_template(\"What is the advantage of running AI Models in {env}\")\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    systemMessage, \n",
    "    humanMessage\n",
    "])\n",
    "\n",
    "# 1 way of doing it\n",
    "# prompt_template = ChatPromptTemplate([\n",
    "#     (\"system\", \"You are an LLM expert\"),\n",
    "#     (\"user\", \"What is the advantage of running AI Models in {env}\")\n",
    "# ])\n",
    "\n",
    "prompt_template\n",
    "\n",
    "prompt = prompt_template.invoke({\"env\": \"local machine\"})\n",
    "\n",
    "# print(prompt)\n",
    "\n",
    "content = llm.invoke(prompt).content\n",
    "\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408cd470",
   "metadata": {},
   "source": [
    "### Message Placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41477471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I'm trying to understand the advantages of running a Large Language Model (LLM) on a local machine. I know that most people use these models online through platforms like ChatGPT or via APIs, but sometimes they might consider running them locally. Let me think about why someone would want to do that.\n",
      "\n",
      "First, maybe privacy is a concern. If I run the model on my own machine, I don't have to send all my inputs and outputs over the internet. That means whatever I'm working on or discussing remains private. This could be important for handling sensitive information or just keeping my conversations more secure.\n",
      "\n",
      "Another thought is about control. When you run an LLM locally, you might have more say in how it's configured. You can choose different versions of the model, maybe even modify or fine-tune them according to specific needs. This customization could make the model more effective for particular tasks, like customer service or creative writing.\n",
      "\n",
      "Cost is another factor. If I'm using a local model, I might not have to pay for access. This is especially useful if I don't need high-speed processing or if I'm experimenting with different models without the financial commitment. Although, I should consider the hardware costs because running some models locally might require powerful GPUs which can be pricey.\n",
      "\n",
      "Performance could also play a role. If my internet connection is slow or unreliable, using an online service might be frustrating. Running the model on my own machine might offer better response times and more reliability since it's not dependent on external factors like server load or latency.\n",
      "\n",
      "Access to niche models might be another benefit. Maybe there are specific versions of LLMs that aren't widely available through online services. By running them locally, I can access these less common models without waiting for them to be released publicly elsewhere.\n",
      "\n",
      "I should also think about community and collaboration. If I'm working on a project with others who have local setups, it might be easier to share models and work together without relying on external systems that might not support real-time collaboration smoothly.\n",
      "\n",
      "There's also the aspect of experimentation. With a local model, I can try out different configurations or modifications more easily since I don't have to worry about upload times or API limits. This could help in developing new techniques or optimizing performance for specific tasks.\n",
      "\n",
      "However, I shouldn't forget the technical challenges. Running large models locally requires significant computational resources. For example, if someone doesn't have a high-end GPU, they might struggle with processing larger models like GPT-3. There's also the issue of memory and storageâ€”storing the model parameters could take up a lot of space.\n",
      "\n",
      "In terms of scalability, managing multiple local instances could be more resource-intensive than relying on centralized cloud-based services. This might not be feasible for large organizations or projects where resources are stretched thin.\n",
      "\n",
      "So, putting it all together, running an LLM locally offers privacy, control over configurations, potential cost savings (though hardware costs must be considered), better performance with reliable connections, access to niche models, easier collaboration within a group, and the ability to experiment freely. However, there are significant technical barriers like resource requirements that need to be addressed.\n",
      "\n",
      "I wonder if there are any other advantages I haven't thought of. Maybe faster processing since data doesn't have to travel over the network? Or the ability to integrate with other tools more seamlessly without latency issues. Also, perhaps better support for multi-language or specialized domains if the model is tailored locally.\n",
      "\n",
      "Wait, but running a local model might require handling different languages or dialects more effectively because the model isn't constrained by the same data sources as online ones. That could be an advantage for certain applications.\n",
      "\n",
      "Another point is that some local setups can allow for real-time processing without waiting for server responses, which can be crucial in time-sensitive applications like automated customer support or live chat systems.\n",
      "\n",
      "I should also consider the community and open-source aspects. Some models might only be available through specific platforms or under certain licenses when accessed online, but running them locally could bypass those restrictions if they're open-source. This could facilitate better research and development without worrying about licensing constraints.\n",
      "\n",
      "In summary, the main advantages seem to revolve around privacy, control, cost-effectiveness (depending on hardware), improved performance, access to specialized models, easier collaboration, faster processing, and potential for more tailored solutions. The disadvantages involve high resource requirements, technical complexity, scalability issues, and possible limitations in accessing certain features or models available online.\n",
      "\n",
      "I think I've covered most aspects, but I might be missing something about community support or ease of debugging since local setups might offer better access to logs and error messages compared to black-box online services.\n",
      "</think>\n",
      "\n",
      "Running a Large Language Model (LLM) on a local machine offers several advantages, each addressing different needs and scenarios:\n",
      "\n",
      "1. **Privacy**: By running the model locally, you maintain control over your data, ensuring that sensitive information remains confidential and not exposed to external servers.\n",
      "\n",
      "2. **Control and Customization**: Local setups allow for more customization, such as using specific model configurations or fine-tuning them for particular tasks, which can enhance performance in specialized domains like customer service or creative writing.\n",
      "\n",
      "3. **Cost-Effectiveness**: Avoiding online services can reduce costs, though hardware expenses must be considered. This is beneficial for experimental use without the financial commitment of continuous access fees.\n",
      "\n",
      "4. **Performance and Reliability**: Local processing eliminates dependencies on external factors like slow internet connections or server load, ensuring more consistent and faster responses.\n",
      "\n",
      "5. **Access to Niche Models**: Accessing less common or experimental models not widely available online can be advantageous for specific applications or research purposes.\n",
      "\n",
      "6. **Collaboration and Experimentation**: Facilitates easier sharing within groups and real-time collaboration, while also allowing for extensive experimentation without concerns about API limits.\n",
      "\n",
      "7. **Faster Processing and Integration**: Local models can process data more quickly and integrate seamlessly with other tools, reducing latency issues in time-sensitive applications.\n",
      "\n",
      "8. **Multi-Language Support and Tailoring**: Potential ability to handle diverse languages or dialects more effectively, useful for specialized domains.\n",
      "\n",
      "9. **Community and Open Source**: Bypasses licensing constraints, allowing access to open-source models and fostering community-driven research and development.\n",
      "\n",
      "10. **Debugging and Support**: Easier access to logs and error messages compared to black-box online services, aiding in troubleshooting and model optimization.\n",
      "\n",
      "However, running LLMs locally also presents challenges, such as high computational and memory requirements, scalability issues for large organizations, and potential limitations in accessing certain features or models available online. Balancing these factors is crucial for determining the best approach based on specific needs and resources.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    MessagesPlaceholder\n",
    ")\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "systemMessage = SystemMessagePromptTemplate.from_template(\"You are an LLM expert\")\n",
    "humanMessage = [HumanMessage(\"What is the advantage of running LLM in local machine\")]\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    systemMessage,\n",
    "    (\"placeholder\", \"{msg}\")\n",
    "])\n",
    "\n",
    "prompt = prompt_template.invoke({\"msg\": humanMessage})\n",
    "\n",
    "content = llm.invoke(prompt).content\n",
    "print(content)\n",
    "# for str in llm.stream(prompt):\n",
    "#      print(str.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
